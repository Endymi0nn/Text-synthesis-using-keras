{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SgL2C70LDleK"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XwMfs_ClDleR"
      },
      "outputs": [],
      "source": [
        "def readfile(path):\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        text_in_string = f.read()\n",
        "    return text_in_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zWUWww57DleR"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm',disable=['parser','tagger','ner'])\n",
        "nlp.max_length= 1200000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B1vt8HNVDleS"
      },
      "outputs": [],
      "source": [
        "def separate_punc(text):\n",
        "    return[word.text.lower() for word in nlp(text) if word.text not in '\\n\\n \\n\\n\\n|\"-#%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n ']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKIBfB9mD4K5",
        "outputId": "d2cfc3c5-d91a-4737-fef0-d000acbbbae3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrii5froELSN",
        "outputId": "7d738199-a909-4f8c-991f-213bb9da18b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpWy6rb5DleS",
        "outputId": "bb81b130-1b70-435e-8c8f-1115481669bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "data=readfile('Dataset.txt')\n",
        "tokens=separate_punc(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQqOGegUDleT",
        "outputId": "75fe1b8b-d05d-4ee8-b5e3-f836bc58a276"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'boy', 'who', 'lived', 'mr.', 'and', 'mrs.', 'dursley', 'of', 'number']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O4cQiNirDleU"
      },
      "outputs": [],
      "source": [
        "train_len=10 + 1\n",
        "text_sequences= []\n",
        "for i in range(train_len,len(tokens)):\n",
        "  seq= tokens[i-train_len:i]\n",
        "  text_sequences.append(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwPou9ioDleU",
        "outputId": "134849e2-1ecf-462c-9a1f-ecbfd4ac7b8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who',\n",
              " 'lived',\n",
              " 'mr.',\n",
              " 'and',\n",
              " 'mrs.',\n",
              " 'dursley',\n",
              " 'of',\n",
              " 'number',\n",
              " 'four',\n",
              " 'privet',\n",
              " 'drive']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "text_sequences[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hQ-uN0QADleV"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences=tokenizer.texts_to_sequences(text_sequences)\n",
        "\n",
        "vocabulary_size=len(tokenizer.word_counts)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XclWdBX8DleV"
      },
      "outputs": [],
      "source": [
        "sequences=np.array(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtN2mA-ODleV",
        "outputId": "5c225db1-6a27-4dd3-fb6e-1c56936dcec5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,  153,   73, 1099,  159,    4,  265,  223,    9,  614,  325],\n",
              "       [ 153,   73, 1099,  159,    4,  265,  223,    9,  614,  325,  615],\n",
              "       [  73, 1099,  159,    4,  265,  223,    9,  614,  325,  615,  506]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "sequences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YWQ5BqvDleW",
        "outputId": "d35f6744-2056-4844-b03a-483b0e9bc7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6172\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMHHlgqLDleW",
        "outputId": "7ac7a5ac-b31b-49de-9d1f-e2c33b2d74b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 153   73 1099  159    4  265  223    9  614  325]\n",
            " [  73 1099  159    4  265  223    9  614  325  615]\n",
            " [1099  159    4  265  223    9  614  325  615  506]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "x = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "seq_len = x.shape[1]\n",
        "y=to_categorical(y,num_classes=vocabulary_size)\n",
        "\n",
        "print(x[1:4])\n",
        "print(y[1:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsFul3UgDleX",
        "outputId": "70bca968-4e18-48b9-8cb4-348f7adf79db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90831, 10)\n",
            "(90831, 6172)\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ufSmY3UdDleX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(tokenizer,open('mysimpletokenizer','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_ngRkCJSDleX"
      },
      "outputs": [],
      "source": [
        "np.savez('processed_data', x, y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}